{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "# plt.rcdefaults()\n",
    "mpl.style.use('additional')\n",
    "\n",
    "def print_df_max_colwidth(df, width=200):\n",
    "    with pd.option_context('display.max_colwidth', width):\n",
    "        print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import SGDClassifier, LogisticRegressionCV\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score, cross_val_predict, train_test_split, StratifiedShuffleSplit\n",
    "from sklearn.metrics import classification_report, confusion_matrix, log_loss\n",
    "from scipy import sparse\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading & Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_df = pd.read_json(\"train.json\")\n",
    "to_test_df = pd.read_json(\"test.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def engineer_features(df):\n",
    "    price_max = np.percentile(df.price, 99)\n",
    "    df.loc[df.price > price_max, 'price'] = price_max\n",
    "    latitude_max = np.percentile(df.latitude, 99)\n",
    "    latitude_min = np.percentile(df.latitude, 1)\n",
    "    df.loc[df.latitude > latitude_max, 'latitude'] = latitude_max\n",
    "    df.loc[df.latitude < latitude_min, 'latitude'] = latitude_min\n",
    "    longitude_max = np.percentile(df.longitude, 99)\n",
    "    longitude_min = np.percentile(df.longitude, 1)\n",
    "    df.loc[df.longitude > longitude_max, 'longitude'] = longitude_max\n",
    "    df.loc[df.longitude < longitude_min, 'longitude'] = longitude_min\n",
    "    df[\"num_photos\"] = df.photos.apply(len)\n",
    "    df[\"num_features\"] = df.features.apply(len)\n",
    "    df[\"num_description_words\"] = df.description.apply(lambda x: len(x.split(\" \")))\n",
    "    df[\"created\"] = pd.to_datetime(df.created)\n",
    "    df[\"created_year\"] = df.created.dt.year\n",
    "    df[\"created_month\"] = df.created.dt.month\n",
    "    df[\"created_day\"] = df.created.dt.day\n",
    "    \n",
    "    df['all_rooms'] = df['bathrooms'] + df['bedrooms']\n",
    "    df['price_per_bed'] = df['price'] / df['bedrooms']    \n",
    "    df['price_per_bath'] = df['price'] / df['bathrooms']\n",
    "    df['price_per_room'] = df['price'] / df['all_rooms']\n",
    "\n",
    "    df['price_per_bed'] = df['price_per_bed'].replace(np.Inf, 5000)\n",
    "    df['price_per_bath'] = df['price_per_bath'].replace(np.Inf, 5000)\n",
    "    df['price_per_room'] = df['price_per_room'].replace(np.Inf, 5000)\n",
    "    \n",
    "engineer_features(train_df)\n",
    "engineer_features(to_test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "\n",
    "def features_cleanup_star(x):\n",
    "    return list(chain.from_iterable(re.sub(r'\\s*\\*+\\s*\\**\\s*', r'*', i).strip('*').split('*') for i in x))\n",
    "\n",
    "def process_features(df):\n",
    "    df['features_clean'] = df['features']\\\n",
    "                    .apply(lambda x: ' '.join([re.sub(r'\\W', '', i) for i in x]).lower())\n",
    "\n",
    "    df.loc[df.features_clean.str.contains('\\*'), 'features_clean'] = \\\n",
    "            df.loc[df.features_clean.str.contains('\\*'), 'features']\\\n",
    "                    .apply(features_cleanup_star)\\\n",
    "                    .apply(lambda x: ' '.join([re.sub(r'\\W', '', i) for i in x]).lower())\n",
    "                \n",
    "process_features(train_df)\n",
    "process_features(to_test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "countvec_features = CountVectorizer(stop_words='english', max_features=200)\n",
    "countvec_features.fit(train_df['features_clean'].tolist() + to_test_df['features_clean'].tolist())\n",
    "train_df_features = countvec_features.transform(train_df['features_clean'])\n",
    "to_test_df_features = countvec_features.transform(to_test_df['features_clean'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "countvec_desc = CountVectorizer(stop_words='english', max_features=200)\n",
    "countvec_desc.fit(train_df['description'].tolist() + to_test_df['description'].tolist())\n",
    "train_df_desc = countvec_desc.transform(train_df['description'])\n",
    "to_test_df_desc = countvec_desc.transform(to_test_df['description'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_vars = ['bathrooms', 'bedrooms', 'latitude', 'longitude', 'price', \n",
    "          'num_photos', 'num_features', 'num_description_words', \n",
    "          'created_year', 'created_month', 'created_day', \n",
    "          'all_rooms', 'price_per_bed', 'price_per_bath', 'price_per_room']\n",
    "# target_num_map = {'high':0, 'medium':1, 'low':2}\n",
    "# train_df['interest_level_coded'] = train_df.interest_level.map(target_num_map)\n",
    "train_idx, test_idx = next(StratifiedShuffleSplit(n_splits=1, test_size=0.25).split(train_df[X_vars], train_df.interest_level))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Numerical Part"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grid search and validate `XGBClassifier`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'learning_rate': 0.2, 'subsample': 0.5}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>param_learning_rate</th>\n",
       "      <th>param_subsample</th>\n",
       "      <th>params</th>\n",
       "      <th>rank_test_score</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split0_train_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split1_train_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split2_train_score</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>std_train_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.260482</td>\n",
       "      <td>0.118512</td>\n",
       "      <td>-0.899855</td>\n",
       "      <td>-0.898677</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>{'learning_rate': 0.01, 'subsample': 0.3}</td>\n",
       "      <td>10</td>\n",
       "      <td>-0.900142</td>\n",
       "      <td>-0.898647</td>\n",
       "      <td>-0.899117</td>\n",
       "      <td>-0.899382</td>\n",
       "      <td>-0.900305</td>\n",
       "      <td>-0.898004</td>\n",
       "      <td>0.059935</td>\n",
       "      <td>0.002005</td>\n",
       "      <td>0.000526</td>\n",
       "      <td>0.000563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.709633</td>\n",
       "      <td>0.118994</td>\n",
       "      <td>-0.900064</td>\n",
       "      <td>-0.898822</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>{'learning_rate': 0.01, 'subsample': 0.5}</td>\n",
       "      <td>11</td>\n",
       "      <td>-0.900217</td>\n",
       "      <td>-0.898547</td>\n",
       "      <td>-0.899356</td>\n",
       "      <td>-0.899650</td>\n",
       "      <td>-0.900619</td>\n",
       "      <td>-0.898268</td>\n",
       "      <td>0.106461</td>\n",
       "      <td>0.008420</td>\n",
       "      <td>0.000527</td>\n",
       "      <td>0.000597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.566909</td>\n",
       "      <td>0.115984</td>\n",
       "      <td>-0.900192</td>\n",
       "      <td>-0.898854</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>{'learning_rate': 0.01, 'subsample': 0.7}</td>\n",
       "      <td>12</td>\n",
       "      <td>-0.900647</td>\n",
       "      <td>-0.898755</td>\n",
       "      <td>-0.899245</td>\n",
       "      <td>-0.899484</td>\n",
       "      <td>-0.900683</td>\n",
       "      <td>-0.898323</td>\n",
       "      <td>0.050328</td>\n",
       "      <td>0.003762</td>\n",
       "      <td>0.000669</td>\n",
       "      <td>0.000479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.336265</td>\n",
       "      <td>0.137082</td>\n",
       "      <td>-0.694868</td>\n",
       "      <td>-0.689976</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>{'learning_rate': 0.05, 'subsample': 0.3}</td>\n",
       "      <td>7</td>\n",
       "      <td>-0.695233</td>\n",
       "      <td>-0.690332</td>\n",
       "      <td>-0.692602</td>\n",
       "      <td>-0.691495</td>\n",
       "      <td>-0.696771</td>\n",
       "      <td>-0.688100</td>\n",
       "      <td>0.162832</td>\n",
       "      <td>0.024859</td>\n",
       "      <td>0.001721</td>\n",
       "      <td>0.001409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.631484</td>\n",
       "      <td>0.121284</td>\n",
       "      <td>-0.696053</td>\n",
       "      <td>-0.690979</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>{'learning_rate': 0.05, 'subsample': 0.5}</td>\n",
       "      <td>8</td>\n",
       "      <td>-0.696485</td>\n",
       "      <td>-0.690984</td>\n",
       "      <td>-0.693926</td>\n",
       "      <td>-0.692855</td>\n",
       "      <td>-0.697747</td>\n",
       "      <td>-0.689096</td>\n",
       "      <td>0.068450</td>\n",
       "      <td>0.007444</td>\n",
       "      <td>0.001590</td>\n",
       "      <td>0.001535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2.609694</td>\n",
       "      <td>0.132450</td>\n",
       "      <td>-0.696976</td>\n",
       "      <td>-0.691845</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>{'learning_rate': 0.05, 'subsample': 0.7}</td>\n",
       "      <td>9</td>\n",
       "      <td>-0.697574</td>\n",
       "      <td>-0.691859</td>\n",
       "      <td>-0.694614</td>\n",
       "      <td>-0.693323</td>\n",
       "      <td>-0.698742</td>\n",
       "      <td>-0.690353</td>\n",
       "      <td>0.113866</td>\n",
       "      <td>0.021888</td>\n",
       "      <td>0.001737</td>\n",
       "      <td>0.001212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2.214132</td>\n",
       "      <td>0.136113</td>\n",
       "      <td>-0.658113</td>\n",
       "      <td>-0.648819</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>{'learning_rate': 0.1, 'subsample': 0.3}</td>\n",
       "      <td>4</td>\n",
       "      <td>-0.658165</td>\n",
       "      <td>-0.648424</td>\n",
       "      <td>-0.656471</td>\n",
       "      <td>-0.651405</td>\n",
       "      <td>-0.659704</td>\n",
       "      <td>-0.646629</td>\n",
       "      <td>0.046932</td>\n",
       "      <td>0.004086</td>\n",
       "      <td>0.001320</td>\n",
       "      <td>0.001970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2.681140</td>\n",
       "      <td>0.134540</td>\n",
       "      <td>-0.659145</td>\n",
       "      <td>-0.649500</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>{'learning_rate': 0.1, 'subsample': 0.5}</td>\n",
       "      <td>5</td>\n",
       "      <td>-0.659014</td>\n",
       "      <td>-0.649404</td>\n",
       "      <td>-0.656890</td>\n",
       "      <td>-0.651528</td>\n",
       "      <td>-0.661532</td>\n",
       "      <td>-0.647568</td>\n",
       "      <td>0.098771</td>\n",
       "      <td>0.008906</td>\n",
       "      <td>0.001897</td>\n",
       "      <td>0.001618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2.601040</td>\n",
       "      <td>0.133662</td>\n",
       "      <td>-0.659808</td>\n",
       "      <td>-0.650022</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>{'learning_rate': 0.1, 'subsample': 0.7}</td>\n",
       "      <td>6</td>\n",
       "      <td>-0.659581</td>\n",
       "      <td>-0.650009</td>\n",
       "      <td>-0.657237</td>\n",
       "      <td>-0.651553</td>\n",
       "      <td>-0.662606</td>\n",
       "      <td>-0.648504</td>\n",
       "      <td>0.091532</td>\n",
       "      <td>0.004268</td>\n",
       "      <td>0.002198</td>\n",
       "      <td>0.001245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2.332261</td>\n",
       "      <td>0.141710</td>\n",
       "      <td>-0.638253</td>\n",
       "      <td>-0.620376</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>{'learning_rate': 0.2, 'subsample': 0.3}</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.639136</td>\n",
       "      <td>-0.621070</td>\n",
       "      <td>-0.635155</td>\n",
       "      <td>-0.621632</td>\n",
       "      <td>-0.640469</td>\n",
       "      <td>-0.618425</td>\n",
       "      <td>0.099334</td>\n",
       "      <td>0.015309</td>\n",
       "      <td>0.002257</td>\n",
       "      <td>0.001398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>4.074082</td>\n",
       "      <td>0.201649</td>\n",
       "      <td>-0.637868</td>\n",
       "      <td>-0.619947</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>{'learning_rate': 0.2, 'subsample': 0.5}</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.637867</td>\n",
       "      <td>-0.620207</td>\n",
       "      <td>-0.634905</td>\n",
       "      <td>-0.621183</td>\n",
       "      <td>-0.640833</td>\n",
       "      <td>-0.618451</td>\n",
       "      <td>0.346382</td>\n",
       "      <td>0.037086</td>\n",
       "      <td>0.002420</td>\n",
       "      <td>0.001131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2.685095</td>\n",
       "      <td>0.142829</td>\n",
       "      <td>-0.638745</td>\n",
       "      <td>-0.620621</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>{'learning_rate': 0.2, 'subsample': 0.7}</td>\n",
       "      <td>3</td>\n",
       "      <td>-0.639755</td>\n",
       "      <td>-0.621255</td>\n",
       "      <td>-0.635588</td>\n",
       "      <td>-0.622128</td>\n",
       "      <td>-0.640894</td>\n",
       "      <td>-0.618482</td>\n",
       "      <td>0.106066</td>\n",
       "      <td>0.012708</td>\n",
       "      <td>0.002281</td>\n",
       "      <td>0.001554</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean_fit_time  mean_score_time  mean_test_score  mean_train_score param_learning_rate param_subsample                                     params  rank_test_score  split0_test_score  split0_train_score  split1_test_score  split1_train_score  split2_test_score  split2_train_score  std_fit_time  std_score_time  std_test_score  std_train_score\n",
       "0        2.260482         0.118512        -0.899855         -0.898677            0.010000        0.300000  {'learning_rate': 0.01, 'subsample': 0.3}               10          -0.900142           -0.898647          -0.899117           -0.899382          -0.900305           -0.898004      0.059935        0.002005        0.000526         0.000563\n",
       "1        2.709633         0.118994        -0.900064         -0.898822            0.010000        0.500000  {'learning_rate': 0.01, 'subsample': 0.5}               11          -0.900217           -0.898547          -0.899356           -0.899650          -0.900619           -0.898268      0.106461        0.008420        0.000527         0.000597\n",
       "2        2.566909         0.115984        -0.900192         -0.898854            0.010000        0.700000  {'learning_rate': 0.01, 'subsample': 0.7}               12          -0.900647           -0.898755          -0.899245           -0.899484          -0.900683           -0.898323      0.050328        0.003762        0.000669         0.000479\n",
       "3        2.336265         0.137082        -0.694868         -0.689976            0.050000        0.300000  {'learning_rate': 0.05, 'subsample': 0.3}                7          -0.695233           -0.690332          -0.692602           -0.691495          -0.696771           -0.688100      0.162832        0.024859        0.001721         0.001409\n",
       "4        2.631484         0.121284        -0.696053         -0.690979            0.050000        0.500000  {'learning_rate': 0.05, 'subsample': 0.5}                8          -0.696485           -0.690984          -0.693926           -0.692855          -0.697747           -0.689096      0.068450        0.007444        0.001590         0.001535\n",
       "5        2.609694         0.132450        -0.696976         -0.691845            0.050000        0.700000  {'learning_rate': 0.05, 'subsample': 0.7}                9          -0.697574           -0.691859          -0.694614           -0.693323          -0.698742           -0.690353      0.113866        0.021888        0.001737         0.001212\n",
       "6        2.214132         0.136113        -0.658113         -0.648819            0.100000        0.300000   {'learning_rate': 0.1, 'subsample': 0.3}                4          -0.658165           -0.648424          -0.656471           -0.651405          -0.659704           -0.646629      0.046932        0.004086        0.001320         0.001970\n",
       "7        2.681140         0.134540        -0.659145         -0.649500            0.100000        0.500000   {'learning_rate': 0.1, 'subsample': 0.5}                5          -0.659014           -0.649404          -0.656890           -0.651528          -0.661532           -0.647568      0.098771        0.008906        0.001897         0.001618\n",
       "8        2.601040         0.133662        -0.659808         -0.650022            0.100000        0.700000   {'learning_rate': 0.1, 'subsample': 0.7}                6          -0.659581           -0.650009          -0.657237           -0.651553          -0.662606           -0.648504      0.091532        0.004268        0.002198         0.001245\n",
       "9        2.332261         0.141710        -0.638253         -0.620376            0.200000        0.300000   {'learning_rate': 0.2, 'subsample': 0.3}                2          -0.639136           -0.621070          -0.635155           -0.621632          -0.640469           -0.618425      0.099334        0.015309        0.002257         0.001398\n",
       "10       4.074082         0.201649        -0.637868         -0.619947            0.200000        0.500000   {'learning_rate': 0.2, 'subsample': 0.5}                1          -0.637867           -0.620207          -0.634905           -0.621183          -0.640833           -0.618451      0.346382        0.037086        0.002420         0.001131\n",
       "11       2.685095         0.142829        -0.638745         -0.620621            0.200000        0.700000   {'learning_rate': 0.2, 'subsample': 0.7}                3          -0.639755           -0.621255          -0.635588           -0.622128          -0.640894           -0.618482      0.106066        0.012708        0.002281         0.001554"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gbc = GridSearchCV(xgb.XGBClassifier(n_estimators=50, objective='multi:softprob'), \n",
    "                   {'learning_rate': [0.01, 0.05, 0.1, 0.2], }, scoring='neg_log_loss')\\\n",
    "            .fit(train_df[X_vars].iloc[train_idx], train_df.interest_level.iloc[train_idx])\n",
    "print(gbc.best_params_)\n",
    "pd.DataFrame(gbc.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'max_depth': 5}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>param_max_depth</th>\n",
       "      <th>params</th>\n",
       "      <th>rank_test_score</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split0_train_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split1_train_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split2_train_score</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>std_train_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8.039676</td>\n",
       "      <td>0.409955</td>\n",
       "      <td>-0.625936</td>\n",
       "      <td>-0.595014</td>\n",
       "      <td>3</td>\n",
       "      <td>{'max_depth': 3}</td>\n",
       "      <td>3</td>\n",
       "      <td>-0.625681</td>\n",
       "      <td>-0.594693</td>\n",
       "      <td>-0.624240</td>\n",
       "      <td>-0.596058</td>\n",
       "      <td>-0.627888</td>\n",
       "      <td>-0.594292</td>\n",
       "      <td>0.275498</td>\n",
       "      <td>0.076090</td>\n",
       "      <td>0.001500</td>\n",
       "      <td>0.000756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13.030548</td>\n",
       "      <td>0.633976</td>\n",
       "      <td>-0.615243</td>\n",
       "      <td>-0.515378</td>\n",
       "      <td>5</td>\n",
       "      <td>{'max_depth': 5}</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.614748</td>\n",
       "      <td>-0.516259</td>\n",
       "      <td>-0.613652</td>\n",
       "      <td>-0.514400</td>\n",
       "      <td>-0.617329</td>\n",
       "      <td>-0.515477</td>\n",
       "      <td>0.054695</td>\n",
       "      <td>0.041242</td>\n",
       "      <td>0.001541</td>\n",
       "      <td>0.000762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>18.429672</td>\n",
       "      <td>0.981492</td>\n",
       "      <td>-0.618669</td>\n",
       "      <td>-0.402830</td>\n",
       "      <td>7</td>\n",
       "      <td>{'max_depth': 7}</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.618880</td>\n",
       "      <td>-0.401774</td>\n",
       "      <td>-0.618492</td>\n",
       "      <td>-0.398189</td>\n",
       "      <td>-0.618636</td>\n",
       "      <td>-0.408529</td>\n",
       "      <td>0.177320</td>\n",
       "      <td>0.019713</td>\n",
       "      <td>0.000160</td>\n",
       "      <td>0.004287</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_fit_time  mean_score_time  mean_test_score  mean_train_score param_max_depth            params  rank_test_score  split0_test_score  split0_train_score  split1_test_score  split1_train_score  split2_test_score  split2_train_score  std_fit_time  std_score_time  std_test_score  std_train_score\n",
       "0       8.039676         0.409955        -0.625936         -0.595014               3  {'max_depth': 3}                3          -0.625681           -0.594693          -0.624240           -0.596058          -0.627888           -0.594292      0.275498        0.076090        0.001500         0.000756\n",
       "1      13.030548         0.633976        -0.615243         -0.515378               5  {'max_depth': 5}                1          -0.614748           -0.516259          -0.613652           -0.514400          -0.617329           -0.515477      0.054695        0.041242        0.001541         0.000762\n",
       "2      18.429672         0.981492        -0.618669         -0.402830               7  {'max_depth': 7}                2          -0.618880           -0.401774          -0.618492           -0.398189          -0.618636           -0.408529      0.177320        0.019713        0.000160         0.004287"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gbc = GridSearchCV(xgb.XGBClassifier(n_estimators=200, objective='multi:softprob'), \n",
    "                   {'max_depth': [3, 5, 7]}, scoring='neg_log_loss')\\\n",
    "            .fit(train_df[X_vars].iloc[train_idx], train_df.interest_level.iloc[train_idx])\n",
    "print(gbc.best_params_)\n",
    "pd.DataFrame(gbc.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'colsample_bytree': 0.7, 'subsample': 0.3}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>param_colsample_bytree</th>\n",
       "      <th>param_subsample</th>\n",
       "      <th>params</th>\n",
       "      <th>rank_test_score</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split0_train_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split1_train_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split2_train_score</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>std_train_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.373880</td>\n",
       "      <td>0.162061</td>\n",
       "      <td>-0.675198</td>\n",
       "      <td>-0.668141</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>{'colsample_bytree': 0.3, 'subsample': 0.3}</td>\n",
       "      <td>9</td>\n",
       "      <td>-0.675846</td>\n",
       "      <td>-0.667723</td>\n",
       "      <td>-0.671948</td>\n",
       "      <td>-0.669790</td>\n",
       "      <td>-0.677802</td>\n",
       "      <td>-0.666911</td>\n",
       "      <td>0.341337</td>\n",
       "      <td>0.047884</td>\n",
       "      <td>0.002433</td>\n",
       "      <td>0.001212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.255084</td>\n",
       "      <td>0.141151</td>\n",
       "      <td>-0.674692</td>\n",
       "      <td>-0.667616</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>{'colsample_bytree': 0.3, 'subsample': 0.5}</td>\n",
       "      <td>7</td>\n",
       "      <td>-0.675617</td>\n",
       "      <td>-0.667360</td>\n",
       "      <td>-0.671238</td>\n",
       "      <td>-0.669445</td>\n",
       "      <td>-0.677220</td>\n",
       "      <td>-0.666043</td>\n",
       "      <td>0.047812</td>\n",
       "      <td>0.022392</td>\n",
       "      <td>0.002528</td>\n",
       "      <td>0.001401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.353777</td>\n",
       "      <td>0.157904</td>\n",
       "      <td>-0.675033</td>\n",
       "      <td>-0.667651</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>{'colsample_bytree': 0.3, 'subsample': 0.7}</td>\n",
       "      <td>8</td>\n",
       "      <td>-0.676761</td>\n",
       "      <td>-0.667423</td>\n",
       "      <td>-0.671026</td>\n",
       "      <td>-0.669067</td>\n",
       "      <td>-0.677313</td>\n",
       "      <td>-0.666465</td>\n",
       "      <td>0.118052</td>\n",
       "      <td>0.014420</td>\n",
       "      <td>0.002843</td>\n",
       "      <td>0.001074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.498730</td>\n",
       "      <td>0.133890</td>\n",
       "      <td>-0.664920</td>\n",
       "      <td>-0.657101</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>{'colsample_bytree': 0.5, 'subsample': 0.3}</td>\n",
       "      <td>5</td>\n",
       "      <td>-0.666236</td>\n",
       "      <td>-0.657503</td>\n",
       "      <td>-0.660686</td>\n",
       "      <td>-0.657986</td>\n",
       "      <td>-0.667837</td>\n",
       "      <td>-0.655815</td>\n",
       "      <td>0.153886</td>\n",
       "      <td>0.006623</td>\n",
       "      <td>0.003064</td>\n",
       "      <td>0.000931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.626641</td>\n",
       "      <td>0.140055</td>\n",
       "      <td>-0.664900</td>\n",
       "      <td>-0.657003</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>{'colsample_bytree': 0.5, 'subsample': 0.5}</td>\n",
       "      <td>4</td>\n",
       "      <td>-0.666057</td>\n",
       "      <td>-0.657479</td>\n",
       "      <td>-0.660767</td>\n",
       "      <td>-0.657866</td>\n",
       "      <td>-0.667876</td>\n",
       "      <td>-0.655666</td>\n",
       "      <td>0.050403</td>\n",
       "      <td>0.015100</td>\n",
       "      <td>0.003015</td>\n",
       "      <td>0.000959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.915306</td>\n",
       "      <td>0.177613</td>\n",
       "      <td>-0.665378</td>\n",
       "      <td>-0.657341</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>{'colsample_bytree': 0.5, 'subsample': 0.7}</td>\n",
       "      <td>6</td>\n",
       "      <td>-0.665846</td>\n",
       "      <td>-0.657158</td>\n",
       "      <td>-0.662325</td>\n",
       "      <td>-0.658710</td>\n",
       "      <td>-0.667963</td>\n",
       "      <td>-0.656154</td>\n",
       "      <td>0.106659</td>\n",
       "      <td>0.022779</td>\n",
       "      <td>0.002325</td>\n",
       "      <td>0.001052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.778154</td>\n",
       "      <td>0.127630</td>\n",
       "      <td>-0.661042</td>\n",
       "      <td>-0.652855</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>{'colsample_bytree': 0.7, 'subsample': 0.3}</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.662248</td>\n",
       "      <td>-0.653493</td>\n",
       "      <td>-0.657987</td>\n",
       "      <td>-0.654465</td>\n",
       "      <td>-0.662891</td>\n",
       "      <td>-0.650607</td>\n",
       "      <td>0.174717</td>\n",
       "      <td>0.002140</td>\n",
       "      <td>0.002176</td>\n",
       "      <td>0.001638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.923207</td>\n",
       "      <td>0.128062</td>\n",
       "      <td>-0.661776</td>\n",
       "      <td>-0.652725</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>{'colsample_bytree': 0.7, 'subsample': 0.5}</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.661931</td>\n",
       "      <td>-0.652561</td>\n",
       "      <td>-0.658557</td>\n",
       "      <td>-0.654114</td>\n",
       "      <td>-0.664840</td>\n",
       "      <td>-0.651501</td>\n",
       "      <td>0.011422</td>\n",
       "      <td>0.006554</td>\n",
       "      <td>0.002567</td>\n",
       "      <td>0.001073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1.893166</td>\n",
       "      <td>0.141564</td>\n",
       "      <td>-0.662276</td>\n",
       "      <td>-0.653384</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>{'colsample_bytree': 0.7, 'subsample': 0.7}</td>\n",
       "      <td>3</td>\n",
       "      <td>-0.662590</td>\n",
       "      <td>-0.653385</td>\n",
       "      <td>-0.659611</td>\n",
       "      <td>-0.655390</td>\n",
       "      <td>-0.664628</td>\n",
       "      <td>-0.651378</td>\n",
       "      <td>0.061233</td>\n",
       "      <td>0.007453</td>\n",
       "      <td>0.002060</td>\n",
       "      <td>0.001638</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_fit_time  mean_score_time  mean_test_score  mean_train_score param_colsample_bytree param_subsample                                       params  rank_test_score  split0_test_score  split0_train_score  split1_test_score  split1_train_score  split2_test_score  split2_train_score  std_fit_time  std_score_time  std_test_score  std_train_score\n",
       "0       1.373880         0.162061        -0.675198         -0.668141               0.300000        0.300000  {'colsample_bytree': 0.3, 'subsample': 0.3}                9          -0.675846           -0.667723          -0.671948           -0.669790          -0.677802           -0.666911      0.341337        0.047884        0.002433         0.001212\n",
       "1       1.255084         0.141151        -0.674692         -0.667616               0.300000        0.500000  {'colsample_bytree': 0.3, 'subsample': 0.5}                7          -0.675617           -0.667360          -0.671238           -0.669445          -0.677220           -0.666043      0.047812        0.022392        0.002528         0.001401\n",
       "2       1.353777         0.157904        -0.675033         -0.667651               0.300000        0.700000  {'colsample_bytree': 0.3, 'subsample': 0.7}                8          -0.676761           -0.667423          -0.671026           -0.669067          -0.677313           -0.666465      0.118052        0.014420        0.002843         0.001074\n",
       "3       1.498730         0.133890        -0.664920         -0.657101               0.500000        0.300000  {'colsample_bytree': 0.5, 'subsample': 0.3}                5          -0.666236           -0.657503          -0.660686           -0.657986          -0.667837           -0.655815      0.153886        0.006623        0.003064         0.000931\n",
       "4       1.626641         0.140055        -0.664900         -0.657003               0.500000        0.500000  {'colsample_bytree': 0.5, 'subsample': 0.5}                4          -0.666057           -0.657479          -0.660767           -0.657866          -0.667876           -0.655666      0.050403        0.015100        0.003015         0.000959\n",
       "5       1.915306         0.177613        -0.665378         -0.657341               0.500000        0.700000  {'colsample_bytree': 0.5, 'subsample': 0.7}                6          -0.665846           -0.657158          -0.662325           -0.658710          -0.667963           -0.656154      0.106659        0.022779        0.002325         0.001052\n",
       "6       1.778154         0.127630        -0.661042         -0.652855               0.700000        0.300000  {'colsample_bytree': 0.7, 'subsample': 0.3}                1          -0.662248           -0.653493          -0.657987           -0.654465          -0.662891           -0.650607      0.174717        0.002140        0.002176         0.001638\n",
       "7       1.923207         0.128062        -0.661776         -0.652725               0.700000        0.500000  {'colsample_bytree': 0.7, 'subsample': 0.5}                2          -0.661931           -0.652561          -0.658557           -0.654114          -0.664840           -0.651501      0.011422        0.006554        0.002567         0.001073\n",
       "8       1.893166         0.141564        -0.662276         -0.653384               0.700000        0.700000  {'colsample_bytree': 0.7, 'subsample': 0.7}                3          -0.662590           -0.653385          -0.659611           -0.655390          -0.664628           -0.651378      0.061233        0.007453        0.002060         0.001638"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gbc = GridSearchCV(xgb.XGBClassifier(n_estimators=50, objective='multi:softprob'), \n",
    "                   {'colsample_bytree': [0.3, 0.5, 0.7], 'subsample': [0.3, 0.5, 0.7]}, scoring='neg_log_loss')\\\n",
    "            .fit(train_df[X_vars].iloc[train_idx], train_df.interest_level.iloc[train_idx])\n",
    "print(gbc.best_params_)\n",
    "pd.DataFrame(gbc.cv_results_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "End up using below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.613285262066\n"
     ]
    }
   ],
   "source": [
    "gbc = xgb.XGBClassifier(max_depth=10, learning_rate=0.1, n_estimators=100, objective='multi:softprob', subsample=0.7)\\\n",
    "         .fit(train_df[X_vars].iloc[train_idx], train_df.interest_level.iloc[train_idx])\n",
    "print(log_loss(train_df.interest_level.iloc[test_idx], gbc.predict_proba(train_df[X_vars].iloc[test_idx])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Text Part"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grid Search and validate `MultinomialNB`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_df_text_combined = sparse.hstack([train_df_features, train_df_desc]).tocsr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'alpha': 10.0}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>param_alpha</th>\n",
       "      <th>params</th>\n",
       "      <th>rank_test_score</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split0_train_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split1_train_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split2_train_score</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>std_train_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.210520</td>\n",
       "      <td>0.079248</td>\n",
       "      <td>-1.136286</td>\n",
       "      <td>-1.090562</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>{'alpha': 0.001}</td>\n",
       "      <td>10</td>\n",
       "      <td>-1.155309</td>\n",
       "      <td>-1.083001</td>\n",
       "      <td>-1.089628</td>\n",
       "      <td>-1.085074</td>\n",
       "      <td>-1.163922</td>\n",
       "      <td>-1.103611</td>\n",
       "      <td>0.026420</td>\n",
       "      <td>0.012483</td>\n",
       "      <td>0.033179</td>\n",
       "      <td>0.009266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.188928</td>\n",
       "      <td>0.071923</td>\n",
       "      <td>-1.133565</td>\n",
       "      <td>-1.090592</td>\n",
       "      <td>0.002783</td>\n",
       "      <td>{'alpha': 0.00278255940221}</td>\n",
       "      <td>9</td>\n",
       "      <td>-1.153012</td>\n",
       "      <td>-1.083014</td>\n",
       "      <td>-1.087132</td>\n",
       "      <td>-1.085128</td>\n",
       "      <td>-1.160552</td>\n",
       "      <td>-1.103634</td>\n",
       "      <td>0.002472</td>\n",
       "      <td>0.003376</td>\n",
       "      <td>0.032977</td>\n",
       "      <td>0.009262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.184816</td>\n",
       "      <td>0.067084</td>\n",
       "      <td>-1.130892</td>\n",
       "      <td>-1.090644</td>\n",
       "      <td>0.007743</td>\n",
       "      <td>{'alpha': 0.00774263682681}</td>\n",
       "      <td>8</td>\n",
       "      <td>-1.150825</td>\n",
       "      <td>-1.083039</td>\n",
       "      <td>-1.084635</td>\n",
       "      <td>-1.085226</td>\n",
       "      <td>-1.157217</td>\n",
       "      <td>-1.103667</td>\n",
       "      <td>0.008400</td>\n",
       "      <td>0.003688</td>\n",
       "      <td>0.032813</td>\n",
       "      <td>0.009252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.209198</td>\n",
       "      <td>0.075314</td>\n",
       "      <td>-1.128211</td>\n",
       "      <td>-1.090717</td>\n",
       "      <td>0.021544</td>\n",
       "      <td>{'alpha': 0.0215443469003}</td>\n",
       "      <td>7</td>\n",
       "      <td>-1.148508</td>\n",
       "      <td>-1.083076</td>\n",
       "      <td>-1.082192</td>\n",
       "      <td>-1.085375</td>\n",
       "      <td>-1.153934</td>\n",
       "      <td>-1.103698</td>\n",
       "      <td>0.002557</td>\n",
       "      <td>0.010877</td>\n",
       "      <td>0.032616</td>\n",
       "      <td>0.009227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.227470</td>\n",
       "      <td>0.071300</td>\n",
       "      <td>-1.125511</td>\n",
       "      <td>-1.090773</td>\n",
       "      <td>0.059948</td>\n",
       "      <td>{'alpha': 0.0599484250319}</td>\n",
       "      <td>6</td>\n",
       "      <td>-1.146064</td>\n",
       "      <td>-1.083099</td>\n",
       "      <td>-1.079795</td>\n",
       "      <td>-1.085539</td>\n",
       "      <td>-1.150675</td>\n",
       "      <td>-1.103680</td>\n",
       "      <td>0.060248</td>\n",
       "      <td>0.002177</td>\n",
       "      <td>0.032381</td>\n",
       "      <td>0.009181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.188014</td>\n",
       "      <td>0.076167</td>\n",
       "      <td>-1.122686</td>\n",
       "      <td>-1.090654</td>\n",
       "      <td>0.166810</td>\n",
       "      <td>{'alpha': 0.16681005372}</td>\n",
       "      <td>5</td>\n",
       "      <td>-1.143347</td>\n",
       "      <td>-1.082967</td>\n",
       "      <td>-1.077424</td>\n",
       "      <td>-1.085562</td>\n",
       "      <td>-1.147287</td>\n",
       "      <td>-1.103432</td>\n",
       "      <td>0.011661</td>\n",
       "      <td>0.005030</td>\n",
       "      <td>0.032045</td>\n",
       "      <td>0.009098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.242460</td>\n",
       "      <td>0.083492</td>\n",
       "      <td>-1.119780</td>\n",
       "      <td>-1.090049</td>\n",
       "      <td>0.464159</td>\n",
       "      <td>{'alpha': 0.464158883361}</td>\n",
       "      <td>4</td>\n",
       "      <td>-1.140499</td>\n",
       "      <td>-1.082500</td>\n",
       "      <td>-1.075221</td>\n",
       "      <td>-1.085048</td>\n",
       "      <td>-1.143619</td>\n",
       "      <td>-1.102599</td>\n",
       "      <td>0.010890</td>\n",
       "      <td>0.002740</td>\n",
       "      <td>0.031533</td>\n",
       "      <td>0.008935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.209612</td>\n",
       "      <td>0.080055</td>\n",
       "      <td>-1.115859</td>\n",
       "      <td>-1.088276</td>\n",
       "      <td>1.291550</td>\n",
       "      <td>{'alpha': 1.29154966501}</td>\n",
       "      <td>3</td>\n",
       "      <td>-1.136823</td>\n",
       "      <td>-1.081222</td>\n",
       "      <td>-1.071901</td>\n",
       "      <td>-1.083062</td>\n",
       "      <td>-1.138854</td>\n",
       "      <td>-1.100543</td>\n",
       "      <td>0.029166</td>\n",
       "      <td>0.003163</td>\n",
       "      <td>0.031094</td>\n",
       "      <td>0.008707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.227095</td>\n",
       "      <td>0.082816</td>\n",
       "      <td>-1.108898</td>\n",
       "      <td>-1.083227</td>\n",
       "      <td>3.593814</td>\n",
       "      <td>{'alpha': 3.5938136638}</td>\n",
       "      <td>2</td>\n",
       "      <td>-1.131564</td>\n",
       "      <td>-1.077183</td>\n",
       "      <td>-1.065103</td>\n",
       "      <td>-1.077728</td>\n",
       "      <td>-1.130026</td>\n",
       "      <td>-1.094769</td>\n",
       "      <td>0.034769</td>\n",
       "      <td>0.007598</td>\n",
       "      <td>0.030974</td>\n",
       "      <td>0.008164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.211974</td>\n",
       "      <td>0.071439</td>\n",
       "      <td>-1.096291</td>\n",
       "      <td>-1.072447</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>{'alpha': 10.0}</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.123255</td>\n",
       "      <td>-1.065362</td>\n",
       "      <td>-1.052198</td>\n",
       "      <td>-1.069263</td>\n",
       "      <td>-1.113418</td>\n",
       "      <td>-1.082716</td>\n",
       "      <td>0.027598</td>\n",
       "      <td>0.005432</td>\n",
       "      <td>0.031436</td>\n",
       "      <td>0.007434</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_fit_time  mean_score_time  mean_test_score  mean_train_score param_alpha                       params  rank_test_score  split0_test_score  split0_train_score  split1_test_score  split1_train_score  split2_test_score  split2_train_score  std_fit_time  std_score_time  std_test_score  std_train_score\n",
       "0       0.210520         0.079248        -1.136286         -1.090562    0.001000             {'alpha': 0.001}               10          -1.155309           -1.083001          -1.089628           -1.085074          -1.163922           -1.103611      0.026420        0.012483        0.033179         0.009266\n",
       "1       0.188928         0.071923        -1.133565         -1.090592    0.002783  {'alpha': 0.00278255940221}                9          -1.153012           -1.083014          -1.087132           -1.085128          -1.160552           -1.103634      0.002472        0.003376        0.032977         0.009262\n",
       "2       0.184816         0.067084        -1.130892         -1.090644    0.007743  {'alpha': 0.00774263682681}                8          -1.150825           -1.083039          -1.084635           -1.085226          -1.157217           -1.103667      0.008400        0.003688        0.032813         0.009252\n",
       "3       0.209198         0.075314        -1.128211         -1.090717    0.021544   {'alpha': 0.0215443469003}                7          -1.148508           -1.083076          -1.082192           -1.085375          -1.153934           -1.103698      0.002557        0.010877        0.032616         0.009227\n",
       "4       0.227470         0.071300        -1.125511         -1.090773    0.059948   {'alpha': 0.0599484250319}                6          -1.146064           -1.083099          -1.079795           -1.085539          -1.150675           -1.103680      0.060248        0.002177        0.032381         0.009181\n",
       "5       0.188014         0.076167        -1.122686         -1.090654    0.166810     {'alpha': 0.16681005372}                5          -1.143347           -1.082967          -1.077424           -1.085562          -1.147287           -1.103432      0.011661        0.005030        0.032045         0.009098\n",
       "6       0.242460         0.083492        -1.119780         -1.090049    0.464159    {'alpha': 0.464158883361}                4          -1.140499           -1.082500          -1.075221           -1.085048          -1.143619           -1.102599      0.010890        0.002740        0.031533         0.008935\n",
       "7       0.209612         0.080055        -1.115859         -1.088276    1.291550     {'alpha': 1.29154966501}                3          -1.136823           -1.081222          -1.071901           -1.083062          -1.138854           -1.100543      0.029166        0.003163        0.031094         0.008707\n",
       "8       0.227095         0.082816        -1.108898         -1.083227    3.593814      {'alpha': 3.5938136638}                2          -1.131564           -1.077183          -1.065103           -1.077728          -1.130026           -1.094769      0.034769        0.007598        0.030974         0.008164\n",
       "9       0.211974         0.071439        -1.096291         -1.072447   10.000000              {'alpha': 10.0}                1          -1.123255           -1.065362          -1.052198           -1.069263          -1.113418           -1.082716      0.027598        0.005432        0.031436         0.007434"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb = GridSearchCV(MultinomialNB(), {'alpha': np.logspace(-3, 1, 10)}, scoring='neg_log_loss')\\\n",
    "            .fit(train_df_text_combined[train_idx], train_df.interest_level.iloc[train_idx])\n",
    "print(nb.best_params_)\n",
    "pd.DataFrame(nb.cv_results_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "End up using below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.12692032\n"
     ]
    }
   ],
   "source": [
    "nb = MultinomialNB(alpha=10)\\\n",
    "         .fit(train_df_text_combined[train_idx], train_df.interest_level.iloc[train_idx])\n",
    "print(log_loss(train_df.interest_level.iloc[test_idx], nb.predict_proba(train_df_text_combined[test_idx])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grid Search and validate `SGDClassifier`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'alpha': 0.001}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>param_alpha</th>\n",
       "      <th>params</th>\n",
       "      <th>rank_test_score</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split0_train_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split1_train_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split2_train_score</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>std_train_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.181878</td>\n",
       "      <td>0.073340</td>\n",
       "      <td>-1.836156</td>\n",
       "      <td>-1.763813</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>{'alpha': 0.0001}</td>\n",
       "      <td>10</td>\n",
       "      <td>-1.912437</td>\n",
       "      <td>-1.831883</td>\n",
       "      <td>-1.949561</td>\n",
       "      <td>-1.913284</td>\n",
       "      <td>-1.646449</td>\n",
       "      <td>-1.546274</td>\n",
       "      <td>0.012826</td>\n",
       "      <td>0.005392</td>\n",
       "      <td>0.134989</td>\n",
       "      <td>0.157372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.174800</td>\n",
       "      <td>0.103093</td>\n",
       "      <td>-0.974159</td>\n",
       "      <td>-0.944993</td>\n",
       "      <td>0.000215</td>\n",
       "      <td>{'alpha': 0.000215443469003}</td>\n",
       "      <td>9</td>\n",
       "      <td>-0.876842</td>\n",
       "      <td>-0.852395</td>\n",
       "      <td>-1.078281</td>\n",
       "      <td>-1.053093</td>\n",
       "      <td>-0.967360</td>\n",
       "      <td>-0.929491</td>\n",
       "      <td>0.008547</td>\n",
       "      <td>0.028620</td>\n",
       "      <td>0.082379</td>\n",
       "      <td>0.082665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.182999</td>\n",
       "      <td>0.079511</td>\n",
       "      <td>-0.812242</td>\n",
       "      <td>-0.789922</td>\n",
       "      <td>0.000464</td>\n",
       "      <td>{'alpha': 0.000464158883361}</td>\n",
       "      <td>7</td>\n",
       "      <td>-0.866304</td>\n",
       "      <td>-0.835097</td>\n",
       "      <td>-0.761933</td>\n",
       "      <td>-0.745402</td>\n",
       "      <td>-0.808486</td>\n",
       "      <td>-0.789268</td>\n",
       "      <td>0.013444</td>\n",
       "      <td>0.012168</td>\n",
       "      <td>0.042693</td>\n",
       "      <td>0.036621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.178709</td>\n",
       "      <td>0.077306</td>\n",
       "      <td>-0.751302</td>\n",
       "      <td>-0.734818</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>{'alpha': 0.001}</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.755007</td>\n",
       "      <td>-0.734498</td>\n",
       "      <td>-0.747953</td>\n",
       "      <td>-0.739454</td>\n",
       "      <td>-0.750945</td>\n",
       "      <td>-0.730502</td>\n",
       "      <td>0.016126</td>\n",
       "      <td>0.004648</td>\n",
       "      <td>0.002891</td>\n",
       "      <td>0.003661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.171885</td>\n",
       "      <td>0.066860</td>\n",
       "      <td>-0.756982</td>\n",
       "      <td>-0.739404</td>\n",
       "      <td>0.002154</td>\n",
       "      <td>{'alpha': 0.00215443469003}</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.771129</td>\n",
       "      <td>-0.748521</td>\n",
       "      <td>-0.746961</td>\n",
       "      <td>-0.735934</td>\n",
       "      <td>-0.752855</td>\n",
       "      <td>-0.733757</td>\n",
       "      <td>0.001552</td>\n",
       "      <td>0.006522</td>\n",
       "      <td>0.010289</td>\n",
       "      <td>0.006508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.174942</td>\n",
       "      <td>0.068327</td>\n",
       "      <td>-0.762057</td>\n",
       "      <td>-0.748879</td>\n",
       "      <td>0.004642</td>\n",
       "      <td>{'alpha': 0.00464158883361}</td>\n",
       "      <td>3</td>\n",
       "      <td>-0.766359</td>\n",
       "      <td>-0.750671</td>\n",
       "      <td>-0.756635</td>\n",
       "      <td>-0.749018</td>\n",
       "      <td>-0.763176</td>\n",
       "      <td>-0.746947</td>\n",
       "      <td>0.005104</td>\n",
       "      <td>0.005661</td>\n",
       "      <td>0.004048</td>\n",
       "      <td>0.001524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.168781</td>\n",
       "      <td>0.073671</td>\n",
       "      <td>-0.773503</td>\n",
       "      <td>-0.763341</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>{'alpha': 0.01}</td>\n",
       "      <td>4</td>\n",
       "      <td>-0.774645</td>\n",
       "      <td>-0.761876</td>\n",
       "      <td>-0.773328</td>\n",
       "      <td>-0.766547</td>\n",
       "      <td>-0.772535</td>\n",
       "      <td>-0.761599</td>\n",
       "      <td>0.002393</td>\n",
       "      <td>0.008573</td>\n",
       "      <td>0.000870</td>\n",
       "      <td>0.002270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.169284</td>\n",
       "      <td>0.066431</td>\n",
       "      <td>-0.785213</td>\n",
       "      <td>-0.777624</td>\n",
       "      <td>0.021544</td>\n",
       "      <td>{'alpha': 0.0215443469003}</td>\n",
       "      <td>5</td>\n",
       "      <td>-0.785243</td>\n",
       "      <td>-0.775806</td>\n",
       "      <td>-0.785087</td>\n",
       "      <td>-0.778826</td>\n",
       "      <td>-0.785309</td>\n",
       "      <td>-0.778241</td>\n",
       "      <td>0.001650</td>\n",
       "      <td>0.004488</td>\n",
       "      <td>0.000093</td>\n",
       "      <td>0.001308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.173103</td>\n",
       "      <td>0.063775</td>\n",
       "      <td>-0.801250</td>\n",
       "      <td>-0.796270</td>\n",
       "      <td>0.046416</td>\n",
       "      <td>{'alpha': 0.0464158883361}</td>\n",
       "      <td>6</td>\n",
       "      <td>-0.802262</td>\n",
       "      <td>-0.795773</td>\n",
       "      <td>-0.801480</td>\n",
       "      <td>-0.796235</td>\n",
       "      <td>-0.800008</td>\n",
       "      <td>-0.796803</td>\n",
       "      <td>0.004597</td>\n",
       "      <td>0.001814</td>\n",
       "      <td>0.000934</td>\n",
       "      <td>0.000421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.174201</td>\n",
       "      <td>0.072648</td>\n",
       "      <td>-0.818672</td>\n",
       "      <td>-0.815675</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>{'alpha': 0.1}</td>\n",
       "      <td>8</td>\n",
       "      <td>-0.819119</td>\n",
       "      <td>-0.814884</td>\n",
       "      <td>-0.820983</td>\n",
       "      <td>-0.816247</td>\n",
       "      <td>-0.815913</td>\n",
       "      <td>-0.815892</td>\n",
       "      <td>0.006905</td>\n",
       "      <td>0.000940</td>\n",
       "      <td>0.002094</td>\n",
       "      <td>0.000577</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_fit_time  mean_score_time  mean_test_score  mean_train_score param_alpha                        params  rank_test_score  split0_test_score  split0_train_score  split1_test_score  split1_train_score  split2_test_score  split2_train_score  std_fit_time  std_score_time  std_test_score  std_train_score\n",
       "0       0.181878         0.073340        -1.836156         -1.763813    0.000100             {'alpha': 0.0001}               10          -1.912437           -1.831883          -1.949561           -1.913284          -1.646449           -1.546274      0.012826        0.005392        0.134989         0.157372\n",
       "1       0.174800         0.103093        -0.974159         -0.944993    0.000215  {'alpha': 0.000215443469003}                9          -0.876842           -0.852395          -1.078281           -1.053093          -0.967360           -0.929491      0.008547        0.028620        0.082379         0.082665\n",
       "2       0.182999         0.079511        -0.812242         -0.789922    0.000464  {'alpha': 0.000464158883361}                7          -0.866304           -0.835097          -0.761933           -0.745402          -0.808486           -0.789268      0.013444        0.012168        0.042693         0.036621\n",
       "3       0.178709         0.077306        -0.751302         -0.734818    0.001000              {'alpha': 0.001}                1          -0.755007           -0.734498          -0.747953           -0.739454          -0.750945           -0.730502      0.016126        0.004648        0.002891         0.003661\n",
       "4       0.171885         0.066860        -0.756982         -0.739404    0.002154   {'alpha': 0.00215443469003}                2          -0.771129           -0.748521          -0.746961           -0.735934          -0.752855           -0.733757      0.001552        0.006522        0.010289         0.006508\n",
       "5       0.174942         0.068327        -0.762057         -0.748879    0.004642   {'alpha': 0.00464158883361}                3          -0.766359           -0.750671          -0.756635           -0.749018          -0.763176           -0.746947      0.005104        0.005661        0.004048         0.001524\n",
       "6       0.168781         0.073671        -0.773503         -0.763341    0.010000               {'alpha': 0.01}                4          -0.774645           -0.761876          -0.773328           -0.766547          -0.772535           -0.761599      0.002393        0.008573        0.000870         0.002270\n",
       "7       0.169284         0.066431        -0.785213         -0.777624    0.021544    {'alpha': 0.0215443469003}                5          -0.785243           -0.775806          -0.785087           -0.778826          -0.785309           -0.778241      0.001650        0.004488        0.000093         0.001308\n",
       "8       0.173103         0.063775        -0.801250         -0.796270    0.046416    {'alpha': 0.0464158883361}                6          -0.802262           -0.795773          -0.801480           -0.796235          -0.800008           -0.796803      0.004597        0.001814        0.000934         0.000421\n",
       "9       0.174201         0.072648        -0.818672         -0.815675    0.100000                {'alpha': 0.1}                8          -0.819119           -0.814884          -0.820983           -0.816247          -0.815913           -0.815892      0.006905        0.000940        0.002094         0.000577"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sgd = GridSearchCV(SGDClassifier(n_jobs=-1, loss='log'), {'alpha': np.logspace(-4, -1, 10)}, scoring='neg_log_loss')\\\n",
    "            .fit(train_df_text_combined[train_idx], train_df.interest_level.iloc[train_idx])\n",
    "print(sgd.best_params_)\n",
    "pd.DataFrame(sgd.cv_results_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "End up using below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.745210826559\n"
     ]
    }
   ],
   "source": [
    "sgd = SGDClassifier(alpha=1e-3, n_jobs=-1, loss='log')\\\n",
    "         .fit(train_df_text_combined[train_idx], train_df.interest_level.iloc[train_idx])\n",
    "print(log_loss(train_df.interest_level.iloc[test_idx], sgd.predict_proba(train_df_text_combined[test_idx])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.687420803746\n"
     ]
    }
   ],
   "source": [
    "gbc_text = xgb.XGBClassifier(max_depth=6, learning_rate=0.1, n_estimators=100, objective='multi:softprob', subsample=0.7)\\\n",
    "         .fit(train_df_text_combined[train_idx], train_df.interest_level.iloc[train_idx])\n",
    "print(log_loss(train_df.interest_level.iloc[test_idx], gbc_text.predict_proba(train_df_text_combined[test_idx])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`SGDClassifier` is more effective than `MultinomialNB`, but `XGBClassifier` is even better. But let's keep things fast and use `SGDClassifier`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stacking Together\n",
    "\n",
    "## Three Methodologies\n",
    "\n",
    "**The first attempt**: use `XGBClassifier` and `SGDClassifier` to get two sets of predicted probs for the training set within `train_df`. Then use `LogisticRegressionCV` to validate against the training set's reponse. Obviously this is not right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.40199251, -0.31352343, -0.25663374, -0.22867747, -0.21817649,\n",
       "       -0.21525915, -0.21464798, -0.21454989, -0.21453925, -0.21453941])"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_train_gbc = gbc.predict_proba(train_df[X_vars].iloc[train_idx])\n",
    "y_pred_train_sgd = sgd.predict_proba(train_df_text_combined[train_idx])\n",
    "lrcv = LogisticRegressionCV(Cs=np.logspace(-3, 1, 10), scoring='neg_log_loss', n_jobs=-1)\\\n",
    "         .fit(np.hstack([y_pred_train_gbc, y_pred_train_sgd]), train_df.interest_level.iloc[train_idx])\n",
    "lrcv.scores_['low'].mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.873679860425\n"
     ]
    }
   ],
   "source": [
    "print(log_loss(train_df.interest_level.iloc[test_idx], lrcv.predict_proba(np.hstack([y_pred_gbc, y_pred_sgd]))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The second attempt**: use `XGBClassifier` and `SGDClassifier` to get two sets of predicted probs for the test set within `train_df`. Then use `LogisticRegressionCV` to validate against the test set's reponse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.56942653, -0.52803948, -0.48904405, -0.46792262, -0.46059575,\n",
       "       -0.45866504, -0.45824157, -0.45817531, -0.4581725 , -0.45817535])"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lrcv = LogisticRegressionCV(Cs=np.logspace(-3, 1, 10), scoring='neg_log_loss', n_jobs=-1)\\\n",
    "         .fit(np.hstack([y_pred_gbc, y_pred_sgd]), train_df.interest_level.iloc[test_idx])\n",
    "lrcv.scores_['low'].mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.61368417946\n"
     ]
    }
   ],
   "source": [
    "print(log_loss(train_df.interest_level.iloc[test_idx], lrcv.predict_proba(np.hstack([y_pred_gbc, y_pred_sgd]))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The third attempt**: use `XGBClassifier` and `SGDClassifier` to get two sets of predicted probs for the training set within `train_df` through cross validation. Then use `LogisticRegressionCV` to validate against the training set's reponse. This is the right way because we won't have test set to validate with like in attempt 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_pred_train_cv_gbc = cross_val_predict(gbc, train_df[X_vars].iloc[train_idx], train_df.interest_level.iloc[train_idx], \n",
    "                                        n_jobs=-1, method='predict_proba')\n",
    "y_pred_train_cv_sgd = cross_val_predict(sgd, train_df_text_combined[train_idx], train_df.interest_level.iloc[train_idx], \n",
    "                                        n_jobs=-1, method='predict_proba')\n",
    "y_pred_gbc = gbc.predict_proba(train_df[X_vars].iloc[test_idx])\n",
    "y_pred_sgd = sgd.predict_proba(train_df_text_combined[test_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.52942097, -0.49304633, -0.47407423, -0.46772584, -0.46620131,\n",
       "       -0.46594499, -0.46592832, -0.46593575, -0.46594105, -0.46594352])"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lrcv = LogisticRegressionCV(Cs=np.logspace(-3, 1, 10), scoring='neg_log_loss', n_jobs=-1)\\\n",
    "         .fit(np.hstack([y_pred_train_cv_gbc, y_pred_train_cv_sgd]), train_df.interest_level.iloc[train_idx])\n",
    "lrcv.scores_['low'].mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.61735239762\n"
     ]
    }
   ],
   "source": [
    "print(log_loss(train_df.interest_level.iloc[test_idx], lrcv.predict_proba(np.hstack([y_pred_gbc, y_pred_sgd]))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other Meta-Models\n",
    "\n",
    "`LogisticRegressionCV` may not be performing too well. Let's experiment on other meta-models.\n",
    "\n",
    "Try another `SGDClassifier` for the meta-model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'alpha': 0.00021544346900318845}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>param_alpha</th>\n",
       "      <th>params</th>\n",
       "      <th>rank_test_score</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split0_train_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split1_train_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split2_train_score</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>std_train_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.163967</td>\n",
       "      <td>0.089536</td>\n",
       "      <td>-0.627055</td>\n",
       "      <td>-0.625334</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>{'alpha': 0.0001}</td>\n",
       "      <td>3</td>\n",
       "      <td>-0.625406</td>\n",
       "      <td>-0.625072</td>\n",
       "      <td>-0.627195</td>\n",
       "      <td>-0.626385</td>\n",
       "      <td>-0.628565</td>\n",
       "      <td>-0.624547</td>\n",
       "      <td>0.011267</td>\n",
       "      <td>0.018212</td>\n",
       "      <td>0.001293</td>\n",
       "      <td>0.000773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.154598</td>\n",
       "      <td>0.071983</td>\n",
       "      <td>-0.625110</td>\n",
       "      <td>-0.623015</td>\n",
       "      <td>0.000215</td>\n",
       "      <td>{'alpha': 0.000215443469003}</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.624096</td>\n",
       "      <td>-0.623150</td>\n",
       "      <td>-0.623562</td>\n",
       "      <td>-0.622762</td>\n",
       "      <td>-0.627673</td>\n",
       "      <td>-0.623133</td>\n",
       "      <td>0.010058</td>\n",
       "      <td>0.002760</td>\n",
       "      <td>0.001825</td>\n",
       "      <td>0.000179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.146369</td>\n",
       "      <td>0.069727</td>\n",
       "      <td>-0.627628</td>\n",
       "      <td>-0.625141</td>\n",
       "      <td>0.000464</td>\n",
       "      <td>{'alpha': 0.000464158883361}</td>\n",
       "      <td>4</td>\n",
       "      <td>-0.625585</td>\n",
       "      <td>-0.624177</td>\n",
       "      <td>-0.631980</td>\n",
       "      <td>-0.626854</td>\n",
       "      <td>-0.625320</td>\n",
       "      <td>-0.624393</td>\n",
       "      <td>0.004317</td>\n",
       "      <td>0.008803</td>\n",
       "      <td>0.003079</td>\n",
       "      <td>0.001214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.146177</td>\n",
       "      <td>0.062691</td>\n",
       "      <td>-0.626631</td>\n",
       "      <td>-0.625356</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>{'alpha': 0.001}</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.625406</td>\n",
       "      <td>-0.624659</td>\n",
       "      <td>-0.625548</td>\n",
       "      <td>-0.625842</td>\n",
       "      <td>-0.628941</td>\n",
       "      <td>-0.625568</td>\n",
       "      <td>0.003861</td>\n",
       "      <td>0.003804</td>\n",
       "      <td>0.001634</td>\n",
       "      <td>0.000505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.151066</td>\n",
       "      <td>0.063591</td>\n",
       "      <td>-0.629674</td>\n",
       "      <td>-0.628703</td>\n",
       "      <td>0.002154</td>\n",
       "      <td>{'alpha': 0.00215443469003}</td>\n",
       "      <td>5</td>\n",
       "      <td>-0.629572</td>\n",
       "      <td>-0.628668</td>\n",
       "      <td>-0.630125</td>\n",
       "      <td>-0.629135</td>\n",
       "      <td>-0.629325</td>\n",
       "      <td>-0.628305</td>\n",
       "      <td>0.007471</td>\n",
       "      <td>0.003549</td>\n",
       "      <td>0.000334</td>\n",
       "      <td>0.000340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.147501</td>\n",
       "      <td>0.063312</td>\n",
       "      <td>-0.638691</td>\n",
       "      <td>-0.638461</td>\n",
       "      <td>0.004642</td>\n",
       "      <td>{'alpha': 0.00464158883361}</td>\n",
       "      <td>6</td>\n",
       "      <td>-0.639728</td>\n",
       "      <td>-0.638540</td>\n",
       "      <td>-0.638808</td>\n",
       "      <td>-0.639133</td>\n",
       "      <td>-0.637537</td>\n",
       "      <td>-0.637710</td>\n",
       "      <td>0.004216</td>\n",
       "      <td>0.000855</td>\n",
       "      <td>0.000898</td>\n",
       "      <td>0.000584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.152194</td>\n",
       "      <td>0.078960</td>\n",
       "      <td>-0.656795</td>\n",
       "      <td>-0.656472</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>{'alpha': 0.01}</td>\n",
       "      <td>7</td>\n",
       "      <td>-0.657586</td>\n",
       "      <td>-0.656415</td>\n",
       "      <td>-0.657242</td>\n",
       "      <td>-0.656405</td>\n",
       "      <td>-0.655557</td>\n",
       "      <td>-0.656597</td>\n",
       "      <td>0.010143</td>\n",
       "      <td>0.021691</td>\n",
       "      <td>0.000886</td>\n",
       "      <td>0.000088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.156853</td>\n",
       "      <td>0.075454</td>\n",
       "      <td>-0.685676</td>\n",
       "      <td>-0.685544</td>\n",
       "      <td>0.021544</td>\n",
       "      <td>{'alpha': 0.0215443469003}</td>\n",
       "      <td>8</td>\n",
       "      <td>-0.686139</td>\n",
       "      <td>-0.685053</td>\n",
       "      <td>-0.686079</td>\n",
       "      <td>-0.685465</td>\n",
       "      <td>-0.684810</td>\n",
       "      <td>-0.686113</td>\n",
       "      <td>0.008088</td>\n",
       "      <td>0.014735</td>\n",
       "      <td>0.000613</td>\n",
       "      <td>0.000436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.154366</td>\n",
       "      <td>0.078454</td>\n",
       "      <td>-0.719098</td>\n",
       "      <td>-0.718993</td>\n",
       "      <td>0.046416</td>\n",
       "      <td>{'alpha': 0.0464158883361}</td>\n",
       "      <td>9</td>\n",
       "      <td>-0.719400</td>\n",
       "      <td>-0.718545</td>\n",
       "      <td>-0.719646</td>\n",
       "      <td>-0.719047</td>\n",
       "      <td>-0.718248</td>\n",
       "      <td>-0.719388</td>\n",
       "      <td>0.007012</td>\n",
       "      <td>0.012590</td>\n",
       "      <td>0.000609</td>\n",
       "      <td>0.000346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.156520</td>\n",
       "      <td>0.151367</td>\n",
       "      <td>-0.747878</td>\n",
       "      <td>-0.747832</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>{'alpha': 0.1}</td>\n",
       "      <td>10</td>\n",
       "      <td>-0.747976</td>\n",
       "      <td>-0.747392</td>\n",
       "      <td>-0.748178</td>\n",
       "      <td>-0.747829</td>\n",
       "      <td>-0.747480</td>\n",
       "      <td>-0.748276</td>\n",
       "      <td>0.003265</td>\n",
       "      <td>0.080113</td>\n",
       "      <td>0.000293</td>\n",
       "      <td>0.000361</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_fit_time  mean_score_time  mean_test_score  mean_train_score param_alpha                        params  rank_test_score  split0_test_score  split0_train_score  split1_test_score  split1_train_score  split2_test_score  split2_train_score  std_fit_time  std_score_time  std_test_score  std_train_score\n",
       "0       0.163967         0.089536        -0.627055         -0.625334    0.000100             {'alpha': 0.0001}                3          -0.625406           -0.625072          -0.627195           -0.626385          -0.628565           -0.624547      0.011267        0.018212        0.001293         0.000773\n",
       "1       0.154598         0.071983        -0.625110         -0.623015    0.000215  {'alpha': 0.000215443469003}                1          -0.624096           -0.623150          -0.623562           -0.622762          -0.627673           -0.623133      0.010058        0.002760        0.001825         0.000179\n",
       "2       0.146369         0.069727        -0.627628         -0.625141    0.000464  {'alpha': 0.000464158883361}                4          -0.625585           -0.624177          -0.631980           -0.626854          -0.625320           -0.624393      0.004317        0.008803        0.003079         0.001214\n",
       "3       0.146177         0.062691        -0.626631         -0.625356    0.001000              {'alpha': 0.001}                2          -0.625406           -0.624659          -0.625548           -0.625842          -0.628941           -0.625568      0.003861        0.003804        0.001634         0.000505\n",
       "4       0.151066         0.063591        -0.629674         -0.628703    0.002154   {'alpha': 0.00215443469003}                5          -0.629572           -0.628668          -0.630125           -0.629135          -0.629325           -0.628305      0.007471        0.003549        0.000334         0.000340\n",
       "5       0.147501         0.063312        -0.638691         -0.638461    0.004642   {'alpha': 0.00464158883361}                6          -0.639728           -0.638540          -0.638808           -0.639133          -0.637537           -0.637710      0.004216        0.000855        0.000898         0.000584\n",
       "6       0.152194         0.078960        -0.656795         -0.656472    0.010000               {'alpha': 0.01}                7          -0.657586           -0.656415          -0.657242           -0.656405          -0.655557           -0.656597      0.010143        0.021691        0.000886         0.000088\n",
       "7       0.156853         0.075454        -0.685676         -0.685544    0.021544    {'alpha': 0.0215443469003}                8          -0.686139           -0.685053          -0.686079           -0.685465          -0.684810           -0.686113      0.008088        0.014735        0.000613         0.000436\n",
       "8       0.154366         0.078454        -0.719098         -0.718993    0.046416    {'alpha': 0.0464158883361}                9          -0.719400           -0.718545          -0.719646           -0.719047          -0.718248           -0.719388      0.007012        0.012590        0.000609         0.000346\n",
       "9       0.156520         0.151367        -0.747878         -0.747832    0.100000                {'alpha': 0.1}               10          -0.747976           -0.747392          -0.748178           -0.747829          -0.747480           -0.748276      0.003265        0.080113        0.000293         0.000361"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sgd_meta = GridSearchCV(SGDClassifier(n_jobs=-1, loss='log'), {'alpha': np.logspace(-4, -1, 10)}, scoring='neg_log_loss')\\\n",
    "            .fit(np.hstack([y_pred_train_cv_gbc, y_pred_train_cv_sgd]), train_df.interest_level.iloc[train_idx])\n",
    "print(sgd_meta.best_params_)\n",
    "pd.DataFrame(sgd_meta.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.619235840253\n"
     ]
    }
   ],
   "source": [
    "sgd_meta = SGDClassifier(alpha=1e-3, n_jobs=-1, loss='log')\\\n",
    "            .fit(np.hstack([y_pred_train_cv_gbc, y_pred_train_cv_sgd]), train_df.interest_level.iloc[train_idx])\n",
    "print(log_loss(train_df.interest_level.iloc[test_idx], sgd_meta.predict_proba(np.hstack([y_pred_gbc, y_pred_sgd]))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try fitting the predicted outcome rather than the probs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.691456951676\n"
     ]
    }
   ],
   "source": [
    "y_pred_train_cv_gbc_1 = cross_val_predict(gbc, train_df[X_vars].iloc[train_idx], train_df.interest_level.iloc[train_idx], \n",
    "                                        n_jobs=-1, method='predict')\n",
    "y_pred_train_cv_sgd_1 = cross_val_predict(sgd, train_df_text_combined[train_idx], train_df.interest_level.iloc[train_idx], \n",
    "                                        n_jobs=-1, method='predict')\n",
    "y_pred_train_cv_combined = pd.get_dummies(pd.DataFrame(np.column_stack([y_pred_train_cv_gbc_1, y_pred_train_cv_sgd_1])))\n",
    "y_pred_gbc_1 = gbc.predict(train_df[X_vars].iloc[test_idx])\n",
    "y_pred_sgd_1 = sgd.predict(train_df_text_combined[test_idx])\n",
    "y_pred_combined = pd.get_dummies(pd.DataFrame(np.column_stack([y_pred_gbc_1, y_pred_sgd_1])))\n",
    "\n",
    "sgd_meta = SGDClassifier(alpha=1e-3, n_jobs=-1, loss='log')\\\n",
    "            .fit(y_pred_train_cv_combined, train_df.interest_level.iloc[train_idx])\n",
    "print(log_loss(train_df.interest_level.iloc[test_idx], sgd_meta.predict_proba(y_pred_combined)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try SVM with `rbf` kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': 10000000000.0, 'gamma': 0.001}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>param_C</th>\n",
       "      <th>param_gamma</th>\n",
       "      <th>params</th>\n",
       "      <th>rank_test_score</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split0_train_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split1_train_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split2_train_score</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>std_train_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>57.857401</td>\n",
       "      <td>3.292474</td>\n",
       "      <td>0.694683</td>\n",
       "      <td>0.694683</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>{'C': 0.01, 'gamma': 1e-09}</td>\n",
       "      <td>3</td>\n",
       "      <td>0.694627</td>\n",
       "      <td>0.694711</td>\n",
       "      <td>0.694683</td>\n",
       "      <td>0.694683</td>\n",
       "      <td>0.694739</td>\n",
       "      <td>0.694655</td>\n",
       "      <td>2.557063</td>\n",
       "      <td>0.148861</td>\n",
       "      <td>0.000046</td>\n",
       "      <td>0.000023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>90.937248</td>\n",
       "      <td>3.978159</td>\n",
       "      <td>0.694683</td>\n",
       "      <td>0.694683</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>{'C': 0.01, 'gamma': 0.001}</td>\n",
       "      <td>3</td>\n",
       "      <td>0.694627</td>\n",
       "      <td>0.694711</td>\n",
       "      <td>0.694683</td>\n",
       "      <td>0.694683</td>\n",
       "      <td>0.694739</td>\n",
       "      <td>0.694655</td>\n",
       "      <td>2.630024</td>\n",
       "      <td>0.119397</td>\n",
       "      <td>0.000046</td>\n",
       "      <td>0.000023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>211.174700</td>\n",
       "      <td>5.865651</td>\n",
       "      <td>0.694683</td>\n",
       "      <td>0.694683</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>{'C': 0.01, 'gamma': 1000.0}</td>\n",
       "      <td>3</td>\n",
       "      <td>0.694627</td>\n",
       "      <td>0.694711</td>\n",
       "      <td>0.694683</td>\n",
       "      <td>0.694683</td>\n",
       "      <td>0.694739</td>\n",
       "      <td>0.694655</td>\n",
       "      <td>25.837990</td>\n",
       "      <td>0.226458</td>\n",
       "      <td>0.000046</td>\n",
       "      <td>0.000023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>51.832420</td>\n",
       "      <td>2.914677</td>\n",
       "      <td>0.694683</td>\n",
       "      <td>0.694683</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>{'C': 10000.0, 'gamma': 1e-09}</td>\n",
       "      <td>3</td>\n",
       "      <td>0.694627</td>\n",
       "      <td>0.694711</td>\n",
       "      <td>0.694683</td>\n",
       "      <td>0.694683</td>\n",
       "      <td>0.694739</td>\n",
       "      <td>0.694655</td>\n",
       "      <td>4.915702</td>\n",
       "      <td>0.343660</td>\n",
       "      <td>0.000046</td>\n",
       "      <td>0.000023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>118.375814</td>\n",
       "      <td>3.601804</td>\n",
       "      <td>0.725347</td>\n",
       "      <td>0.725550</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>{'C': 10000.0, 'gamma': 0.001}</td>\n",
       "      <td>2</td>\n",
       "      <td>0.722587</td>\n",
       "      <td>0.727011</td>\n",
       "      <td>0.725239</td>\n",
       "      <td>0.725361</td>\n",
       "      <td>0.728216</td>\n",
       "      <td>0.724278</td>\n",
       "      <td>3.377007</td>\n",
       "      <td>0.028662</td>\n",
       "      <td>0.002299</td>\n",
       "      <td>0.001124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1235.056354</td>\n",
       "      <td>4.788669</td>\n",
       "      <td>0.619063</td>\n",
       "      <td>0.998055</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>{'C': 10000.0, 'gamma': 1000.0}</td>\n",
       "      <td>9</td>\n",
       "      <td>0.623957</td>\n",
       "      <td>0.997812</td>\n",
       "      <td>0.621414</td>\n",
       "      <td>0.998784</td>\n",
       "      <td>0.611818</td>\n",
       "      <td>0.997569</td>\n",
       "      <td>297.746820</td>\n",
       "      <td>0.094532</td>\n",
       "      <td>0.005227</td>\n",
       "      <td>0.000525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>41.359506</td>\n",
       "      <td>2.662345</td>\n",
       "      <td>0.689712</td>\n",
       "      <td>0.691103</td>\n",
       "      <td>10000000000.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>{'C': 10000000000.0, 'gamma': 1e-09}</td>\n",
       "      <td>7</td>\n",
       "      <td>0.693573</td>\n",
       "      <td>0.689200</td>\n",
       "      <td>0.701410</td>\n",
       "      <td>0.690347</td>\n",
       "      <td>0.674151</td>\n",
       "      <td>0.693763</td>\n",
       "      <td>0.312963</td>\n",
       "      <td>0.083005</td>\n",
       "      <td>0.011458</td>\n",
       "      <td>0.001938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>240.686356</td>\n",
       "      <td>2.846831</td>\n",
       "      <td>0.731021</td>\n",
       "      <td>0.730859</td>\n",
       "      <td>10000000000.000000</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>{'C': 10000000000.0, 'gamma': 0.001}</td>\n",
       "      <td>1</td>\n",
       "      <td>0.730124</td>\n",
       "      <td>0.731307</td>\n",
       "      <td>0.730507</td>\n",
       "      <td>0.730953</td>\n",
       "      <td>0.732431</td>\n",
       "      <td>0.730316</td>\n",
       "      <td>5.271223</td>\n",
       "      <td>0.013222</td>\n",
       "      <td>0.001009</td>\n",
       "      <td>0.000410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1764.622503</td>\n",
       "      <td>4.450449</td>\n",
       "      <td>0.627600</td>\n",
       "      <td>0.998946</td>\n",
       "      <td>10000000000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>{'C': 10000000000.0, 'gamma': 1000.0}</td>\n",
       "      <td>8</td>\n",
       "      <td>0.628819</td>\n",
       "      <td>0.998784</td>\n",
       "      <td>0.619306</td>\n",
       "      <td>0.999352</td>\n",
       "      <td>0.634676</td>\n",
       "      <td>0.998703</td>\n",
       "      <td>515.995170</td>\n",
       "      <td>0.157602</td>\n",
       "      <td>0.006334</td>\n",
       "      <td>0.000288</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_fit_time  mean_score_time  mean_test_score  mean_train_score            param_C param_gamma                                 params  rank_test_score  split0_test_score  split0_train_score  split1_test_score  split1_train_score  split2_test_score  split2_train_score  std_fit_time  std_score_time  std_test_score  std_train_score\n",
       "0      57.857401         3.292474         0.694683          0.694683           0.010000    0.000000            {'C': 0.01, 'gamma': 1e-09}                3           0.694627            0.694711           0.694683            0.694683           0.694739            0.694655      2.557063        0.148861        0.000046         0.000023\n",
       "1      90.937248         3.978159         0.694683          0.694683           0.010000    0.001000            {'C': 0.01, 'gamma': 0.001}                3           0.694627            0.694711           0.694683            0.694683           0.694739            0.694655      2.630024        0.119397        0.000046         0.000023\n",
       "2     211.174700         5.865651         0.694683          0.694683           0.010000 1000.000000           {'C': 0.01, 'gamma': 1000.0}                3           0.694627            0.694711           0.694683            0.694683           0.694739            0.694655     25.837990        0.226458        0.000046         0.000023\n",
       "3      51.832420         2.914677         0.694683          0.694683       10000.000000    0.000000         {'C': 10000.0, 'gamma': 1e-09}                3           0.694627            0.694711           0.694683            0.694683           0.694739            0.694655      4.915702        0.343660        0.000046         0.000023\n",
       "4     118.375814         3.601804         0.725347          0.725550       10000.000000    0.001000         {'C': 10000.0, 'gamma': 0.001}                2           0.722587            0.727011           0.725239            0.725361           0.728216            0.724278      3.377007        0.028662        0.002299         0.001124\n",
       "5    1235.056354         4.788669         0.619063          0.998055       10000.000000 1000.000000        {'C': 10000.0, 'gamma': 1000.0}                9           0.623957            0.997812           0.621414            0.998784           0.611818            0.997569    297.746820        0.094532        0.005227         0.000525\n",
       "6      41.359506         2.662345         0.689712          0.691103 10000000000.000000    0.000000   {'C': 10000000000.0, 'gamma': 1e-09}                7           0.693573            0.689200           0.701410            0.690347           0.674151            0.693763      0.312963        0.083005        0.011458         0.001938\n",
       "7     240.686356         2.846831         0.731021          0.730859 10000000000.000000    0.001000   {'C': 10000000000.0, 'gamma': 0.001}                1           0.730124            0.731307           0.730507            0.730953           0.732431            0.730316      5.271223        0.013222        0.001009         0.000410\n",
       "8    1764.622503         4.450449         0.627600          0.998946 10000000000.000000 1000.000000  {'C': 10000000000.0, 'gamma': 1000.0}                8           0.628819            0.998784           0.619306            0.999352           0.634676            0.998703    515.995170        0.157602        0.006334         0.000288"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm_meta = GridSearchCV(SVC(kernel='rbf', decision_function_shape='ovr', probability=True), \n",
    "                       {'C': np.logspace(-2, 10, 3), 'gamma': np.logspace(-9, 3, 3)})\\\n",
    "            .fit(np.hstack([y_pred_train_cv_gbc, y_pred_train_cv_sgd]), train_df.interest_level.iloc[train_idx])\n",
    "print(svm_meta.best_params_)\n",
    "pd.DataFrame(svm_meta.cv_results_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "End up using below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.608465397572\n"
     ]
    }
   ],
   "source": [
    "svm_meta = SVC(kernel='rbf', decision_function_shape='ovr', probability=True, C=10000000000.0, gamma=1e-3)\\\n",
    "            .fit(np.hstack([y_pred_train_cv_gbc, y_pred_train_cv_sgd]), train_df.interest_level.iloc[train_idx])\n",
    "print(log_loss(train_df.interest_level.iloc[test_idx], svm_meta.predict_proba(np.hstack([y_pred_gbc, y_pred_sgd]))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try `XGBClassifier` again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.624539737738\n"
     ]
    }
   ],
   "source": [
    "gbc_meta = xgb.XGBClassifier(max_depth=10, learning_rate=0.1, n_estimators=100, objective='multi:softprob', subsample=0.7)\\\n",
    "            .fit(np.hstack([y_pred_train_cv_gbc, y_pred_train_cv_sgd]), train_df.interest_level.iloc[train_idx])\n",
    "print(log_loss(train_df.interest_level.iloc[test_idx], gbc_meta.predict_proba(np.hstack([y_pred_gbc, y_pred_sgd]))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Semi-Stacked Model\n",
    "\n",
    "Not too much of an improvement. Try wrapping the probs outcome of `SGDClassifier` for the text portion as extra features for one single `XGBClassifier` model. This is essentially a semi-stacked model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.612064452084\n"
     ]
    }
   ],
   "source": [
    "gbc_semi = xgb.XGBClassifier(max_depth=6, learning_rate=0.1, n_estimators=500, objective='multi:softprob', subsample=0.7)\\\n",
    "         .fit(np.hstack([train_df[X_vars].iloc[train_idx], y_pred_train_cv_sgd]), train_df.interest_level.iloc[train_idx])\n",
    "print(log_loss(train_df.interest_level.iloc[test_idx], gbc_semi.predict_proba(np.hstack([train_df[X_vars].iloc[test_idx], y_pred_sgd]))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is not that great. Try adding predicted probs from both models to all features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_df_with_text = sparse.hstack([train_df[X_vars], train_df_features, train_df_desc]).tocsr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.579410067619\n"
     ]
    }
   ],
   "source": [
    "gbc_semi = xgb.XGBClassifier(max_depth=10, learning_rate=0.1, n_estimators=100, objective='multi:softprob', subsample=0.7)\\\n",
    "         .fit(sparse.hstack([train_df_with_text[train_idx], y_pred_train_cv_gbc, y_pred_train_cv_sgd]).tocsr(), train_df.interest_level.iloc[train_idx])\n",
    "print(log_loss(train_df.interest_level.iloc[test_idx], gbc_semi.predict_proba(sparse.hstack([train_df_with_text[test_idx], y_pred_gbc, y_pred_sgd]).tocsr())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hmm. So using one single model is better?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "to_test_df_with_text = sparse.hstack([to_test_df[X_vars], to_test_df_features, to_test_df_desc]).tocsr()\n",
    "to_test_df_text_combined = sparse.hstack([to_test_df_features, to_test_df_desc]).tocsr()\n",
    "to_test_y_pred_gbc = gbc.predict_proba(to_test_df[X_vars])\n",
    "to_test_y_pred_sgd = sgd.predict_proba(to_test_df_text_combined)\n",
    "to_test_y = gbc_semi.predict_proba(sparse.hstack([to_test_df_with_text, to_test_y_pred_gbc, to_test_y_pred_sgd]).tocsr())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(to_test_y, index=to_test_df.listing_id, columns=['high', 'low', 'medium'])[['high', 'medium', 'low']].to_csv('submission_xgb_semistack.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
